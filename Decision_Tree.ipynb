{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Questions**"
      ],
      "metadata": {
        "id": "jQNKPcaUJwvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        ">  A decision tree is a supervised learning algorithm used for both classification and regression tasks. It has a hierarchical tree structure  consists of a root node, branches, internal nodes and leaf nodes.\n",
        "\n",
        "> A decision tree recursively splits the dataset based on feature values to homogenous subsets. Each leaf node of the tree corresponds to a class label and the internal nodes are feature-based decision points"
      ],
      "metadata": {
        "id": "wVaWBvIUKITV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "> Gini Impurity and Entropy both measures of purity of node in a decision tree, where lower impurity indicates a better split.\n",
        "\n",
        "> The split depends on in which we give us more infromation gain."
      ],
      "metadata": {
        "id": "aifw6SMZJ9DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "> **Pre-pruning** stops a decision tree's growth during training to prevent overfitting. The advantage of pre-pruning is its efficiency, as it prevents the creation of a large, complex tree, saving computational resources.\n",
        "\n",
        ">**Post-pruning** removes unnecessary branches after the tree has fully grown. The advantage is the accuracy, as it considers the entire tree to make more informed decisions about removing subtrees, potentially leading to a more optimized model."
      ],
      "metadata": {
        "id": "AgY8wKKBYNba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "> Information Gain in decision trees is the measure of how much a feature reduces the impurity in the data after a split.\n",
        "\n",
        "> It's calculated as the difference between the entropy of the parent node and the weighted average of the entropies of the child nodes.\n",
        "\n",
        "> Features with the highest Information Gain are chosen for splitting because they provide the most information about the class labels, leading to more homogeneous and accurate decision tree nodes."
      ],
      "metadata": {
        "id": "3d0_sfJ6YWVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**\n",
        "\n",
        "> Decision Trees are used in various fields, including healthcare for diagnosis, finance for risk assessment and fraud detection, and marketing for customer segmentation and churn prediction.\n",
        "\n",
        "**Main Advantages**\n",
        "\n",
        "  * Simplicity\n",
        "  * Data Preparation\n",
        "  * Mixed Data Types\n",
        "  * Feature Importance.\n",
        "  * Computational Efficiency\n",
        "\n",
        "**Main Limitations**\n",
        "  * Overfitting\n",
        "  * Instability\n",
        "  * Biased with Imbalanced Data."
      ],
      "metadata": {
        "id": "OXdEXm9Jacz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgl6wHZxJFNI",
        "outputId": "25b2b310-28e1-4afc-d400-d12cebf7758a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.00\n",
            "sepal width (cm): 0.02\n",
            "petal length (cm): 0.89\n",
            "petal width (cm): 0.09\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Question 6 : Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree Classifier using the Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# 3. Print the model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(feature_names, dt_classifier.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its\n",
        "accuracy to a fully-grown tree.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "dt_classifier_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_classifier_3.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for the limited tree\n",
        "y_pred_3 = dt_classifier_3.predict(X_test)\n",
        "accuracy_3 = accuracy_score(y_test, y_pred_3)\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_3:.2f}\")\n",
        "\n",
        "if accuracy_3 > accuracy:\n",
        "    print(\"The Decision Tree with max_depth=3 performed better.\")\n",
        "elif accuracy > accuracy_3:\n",
        "    print(\"The fully-grown Decision Tree performed better.\")\n",
        "else:\n",
        "    print(\"Both Decision Trees achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiR-GfD1bXFd",
        "outputId": "98e998da-e837-4cb9-d4f7-4e627de3012e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.00\n",
            "Both Decision Trees achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing Dataset as its showing error due to some ethical issue\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "feature_importances = pd.Series(dt_regressor.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1wncIN8bWmj",
        "outputId": "0afc6fb9-fabe-44e0-af74-9552dabda75d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "\n",
            "Feature Importances:\n",
            "MedInc        0.528509\n",
            "AveOccup      0.130838\n",
            "Latitude      0.093717\n",
            "Longitude     0.082902\n",
            "AveRooms      0.052975\n",
            "HouseAge      0.051884\n",
            "Population    0.030516\n",
            "AveBedrms     0.028660\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 9:\n",
        "Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 2. Tune the Decision Tree's max_depth and min_samples_split using GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Print the best parameters and the resulting model accuracy\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best estimator (model)\n",
        "best_dtc = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_dtc.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best model on the test set: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kGRJNoygAKD",
        "outputId": "84a20a23-c245-4f53-892a-4124f60dffe4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'min_samples_split': 10}\n",
            "Accuracy of the best model on the test set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:**\n",
        "  * Handle the missing values\n",
        "  * Encode the categorical features\n",
        "  * Train a Decision Tree model\n",
        "  * Tune its hyperparameters\n",
        "  * Evaluate its performance\n",
        "\n",
        "**And describe what business value this model could provide in the real-world\n",
        "setting.**\n",
        "\n",
        "1. **Handle Missing Values**: First, understand the nature of missing data. Are they random, or is there a pattern? How much data is missing?\n",
        "\n",
        "Choose a Strategy:\n",
        "* Imputation\n",
        "* Removal\n",
        "\n",
        "2. **Encode Categorical Features**\n",
        "* Label Encoding: For nominal (no inherent order) categories, assign a unique integer to each category.\n",
        "* One-Hot Encoding: For nominal categories, create new binary (0 or 1) columns for each unique category.\n",
        "* Ordinal Encoding: For ordinal (ordered) categories (e.g., severity levels), assign integers that reflect their inherent order.\n",
        "\n",
        "3. **Train a Decision Tree Model**\n",
        "\n",
        "* Data Splitting: Divide the dataset into training, validation, and testing sets.\n",
        "* Model Initialization: Instantiate a Decision Tree classifier from a suitable machine learning library.\n",
        "* Training: Fit the Decision Tree model to the training data, allowing it to learn the patterns and relationships that predict the presence of the disease. The model will recursively split the data based on the most informative features to create decision rules.\n",
        "\n",
        "4. **Tune Its Hyperparameters**\n",
        "* Identify Hyperparameters: Decision Trees have hyperparameters like max_depth (maximum tree depth), min_samples_split (minimum samples to split a node), and min_samples_leaf (minimum samples in a leaf node).\n",
        "* Cross-Validation: Use techniques like k-fold cross-validation on the training data to evaluate different hyperparameter settings without biasing the test set.\n",
        "* Hyperparameter Tuning: Employ methods like Grid Search or Random Search to systematically find the best combination of hyperparameters that optimizes model performance and generalization.\n",
        "\n",
        "5. **Evaluate Its Performance**\n",
        "* Metrics: Use various metrics to assess the model's effectiveness on the unseen test set:\n",
        "* Accuracy: The overall percentage of correct predictions.\n",
        "* Precision: Of the predicted positive cases, how many were actually positive.\n",
        "* Recall (Sensitivity): Of the actual positive cases, how many were correctly identified.\n",
        "* F1-Score: A harmonic mean of precision and recall, useful for imbalanced datasets.\n",
        "* Confusion Matrix: A table showing true positives, true negatives, false positives, and false negatives to give a detailed view of model performance.\n",
        "\n",
        "**This model help us to detect the disease early and can provide early diagnosis and can provide the personalized medication and treatment.**\n"
      ],
      "metadata": {
        "id": "QJJ98BDPj24u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXfXm_P_hN_f"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}